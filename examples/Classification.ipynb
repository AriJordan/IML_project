{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "The goal of classification is to find a function that separates the data into positive/negative labels. In the case of a linear classifier, this reduces to finding a set of parameters $w^\\star$ such that, $$ \\begin{align} w^\\star &= \\arg \\min_w \\sum_{i=1}^{N} \\left[y_i\\neq \\text{sign} (w^\\top x_i) \\right] \\\\ &= \\arg \\min_w \\sum_{i=1}^{N} l_{0/1} (w; x_i, y_i) \\end{align}.$$ \n",
    "\n",
    "The problem with the $l_{0/1}$ loss, is that it is non-convex (and non-differentiable), hence other surrogate losses must be used to optimize the number of points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from util import gradient_descent, generate_linear_separable_data, generate_circular_separable_data\n",
    "import plot_helpers\n",
    "import IPython\n",
    "\n",
    "% load_ext autoreload\n",
    "% autoreload 2\n",
    "import ipywidgets\n",
    "from ipywidgets import interact, interactive, interact_manual\n",
    "\n",
    "import pylab\n",
    "pylab.rcParams['figure.figsize'] = (10, 5)\n",
    "\n",
    "from classifiers import Perceptron, SVM, Logistic\n",
    "from regularizers import L1Regularizer, L2Regularizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_points = 100  # Number of points per class\n",
    "noise = 0.5  # Noise Level (needed for data generation).\n",
    "\n",
    "X, Y = generate_linear_separable_data(num_points, noise=noise, dim=2)\n",
    "\n",
    "fig = plt.subplot(111)\n",
    "opt = {'marker': 'ro', 'label': '+', 'size': 8}\n",
    "plot_helpers.plot_data(X[np.where(Y == 1)[0], 0], X[np.where(Y == 1)[0], 1], fig=fig, options=opt)\n",
    "opt = {'marker': 'bs', 'label': '-', 'x_label': '$x$', 'y_label': '$y$', 'size': 8, 'legend': True}\n",
    "plot_helpers.plot_data(X[np.where(Y == -1)[0], 0], X[np.where(Y == -1)[0], 1], fig=fig, options=opt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate into train and test sets!\n",
    "np.random.seed(42)\n",
    "indexes = np.arange(0, 2*num_points, 1)\n",
    "np.random.shuffle(indexes)\n",
    "num_train = int(np.ceil(2*.05*num_points))\n",
    "\n",
    "X_train = X[indexes[:num_train]]\n",
    "Y_train = Y[indexes[:num_train]]\n",
    "\n",
    "X_test = X[indexes[num_train:]]\n",
    "Y_test = Y[indexes[num_train:]]\n",
    "\n",
    "fig = plt.subplot(111)\n",
    "\n",
    "opt = {'marker': 'ro', 'fillstyle': 'full', 'label': '+ Train', 'size': 8}\n",
    "plot_helpers.plot_data(X_train[np.where(Y_train == 1)[0], 0], X_train[np.where(Y_train == 1)[0], 1], fig=fig, options=opt)\n",
    "opt = {'marker': 'bs', 'fillstyle': 'full', 'label': '- Train', 'size': 8}\n",
    "plot_helpers.plot_data(X_train[np.where(Y_train == -1)[0], 0], X_train[np.where(Y_train == -1)[0], 1], fig=fig, options=opt)\n",
    "\n",
    "opt = {'marker': 'ro', 'fillstyle': 'none', 'label': '+ Test', 'size': 8}\n",
    "plot_helpers.plot_data(X_test[np.where(Y_test == 1)[0], 0], X_test[np.where(Y_test == 1)[0], 1], fig=fig, options=opt)\n",
    "opt = {'marker': 'bs', 'fillstyle': 'none', 'label': '- Test', 'size': 8, \n",
    "       'x_label': '$x$', 'y_label': '$y$', 'legend': True}\n",
    "plot_helpers.plot_data(X_test[np.where(Y_test == -1)[0], 0], X_test[np.where(Y_test == -1)[0], 1], fig=fig, options=opt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Perceptron Algorithm\n",
    "\n",
    "The perceptron loss is defined as: $$L(w; X, Y) =  \\sum_{i=1}^{N} L_p(w; x_i, y_i) = \\sum_{i=1}^{N} \\max \\{ 0, -y_i w^\\top x_i \\}.$$\n",
    "\n",
    "The loss function is continuous, but not differentialbe at $y_i w^\\top x_i=0$. The subgradient, however, exists and hence (stochastic) gradient descent converges. The subgradient is:\n",
    "\n",
    "$$ \\partial L_p(w; x_i,y_i) = \\left\\{\\begin{array}{cc} 0 & \\text{if } -y_i w^\\top x_i < 0 \\\\ -y_i x_i & \\text{if } -y_i w^\\top x_i > 0 \\\\ \\left[0, -y_i x_i \\right] & \\text{if } -y_i w^\\top x_i = 0 \\end{array}  \\right. $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_learning_params(reg, eta0, n_iter, batch_size):\n",
    "    classifier = Perceptron(X_train, Y_train)\n",
    "    classifier.load_test_data(X_test, Y_test)\n",
    "    \n",
    "    regularizer = L2Regularizer(np.power(10., reg))\n",
    "    np.random.seed(42)\n",
    "    w0 = np.random.randn(3, )\n",
    "\n",
    "    opts = {'eta0': eta0,\n",
    "            'n_iter': n_iter,\n",
    "            'batch_size': batch_size,\n",
    "            'n_samples': X_train.shape[0],\n",
    "            'algorithm': 'SGD',\n",
    "            'learning_rate_scheduling': None,\n",
    "            }\n",
    "    trajectory, indexes = gradient_descent(w0, classifier, regularizer, opts)\n",
    "\n",
    "    contour_plot = plt.subplot(121)\n",
    "    error_plot = plt.subplot(122)\n",
    "    \n",
    "    opt = {'marker': 'ro', 'fillstyle': 'full', 'label': '+ Train', 'size': 8}\n",
    "    plot_helpers.plot_data(X_train[np.where(Y_train == 1)[0], 0], X_train[np.where(Y_train == 1)[0], 1], fig=contour_plot, options=opt)\n",
    "    opt = {'marker': 'bs', 'fillstyle': 'full', 'label': '- Train', 'size': 8}\n",
    "    plot_helpers.plot_data(X_train[np.where(Y_train == -1)[0], 0], X_train[np.where(Y_train == -1)[0], 1], fig=contour_plot, options=opt)\n",
    "\n",
    "    opt = {'marker': 'ro', 'fillstyle': 'none', 'label': '+ Test', 'size': 8}\n",
    "    plot_helpers.plot_data(X_test[np.where(Y_test == 1)[0], 0], X_test[np.where(Y_test == 1)[0], 1], fig=contour_plot, options=opt)\n",
    "    opt = {'marker': 'bs', 'fillstyle': 'none', 'label': '- Test', 'size': 8}\n",
    "    plot_helpers.plot_data(X_test[np.where(Y_test == -1)[0], 0], X_test[np.where(Y_test == -1)[0], 1], fig=contour_plot, options=opt)\n",
    "\n",
    "    contour_opts = {'n_points': 50, 'x_label': '$x$', 'y_label': '$y$', 'sgd_point': True, 'n_classes': 4}\n",
    "    error_opts = {'epoch': 5, 'x_label': '$t$', 'y_label': 'error'}\n",
    "    \n",
    "    opts = {'contour_opts': contour_opts, 'error_opts': error_opts}\n",
    "    plot_helpers.classification_progression(X, Y, trajectory, indexes, classifier, \n",
    "                                            contour_plot=contour_plot, error_plot=error_plot, \n",
    "                                            options=opts)\n",
    "\n",
    "interact_manual(change_learning_params,\n",
    "                reg=ipywidgets.FloatSlider(value=-3,\n",
    "                                    min=-3,\n",
    "                                    max=3,\n",
    "                                    step=0.5,\n",
    "                                    readout_format='.1f',\n",
    "                                    description='Regularization 10^:',\n",
    "                                    style={'description_width': 'initial'},\n",
    "                                    continuous_update=False), \n",
    "                eta0=ipywidgets.FloatSlider(value=1,\n",
    "                                            min=1e-1,\n",
    "                                            max=2,\n",
    "                                            step=1 * 1e-1,\n",
    "                                            readout_format='.1f',\n",
    "                                            description='Learning rate:',\n",
    "                                            style={'description_width': 'initial'},\n",
    "                                            continuous_update=False),\n",
    "                n_iter=ipywidgets.IntSlider(value=100,\n",
    "                                            min=5,\n",
    "                                            max=100,\n",
    "                                            step=1,\n",
    "                                            description='Number of iterations:',\n",
    "                                            style={'description_width': 'initial'},\n",
    "                                            continuous_update=False),\n",
    "                batch_size=ipywidgets.IntSlider(value=1,\n",
    "                                            min=1,\n",
    "                                            max=X_train.shape[0],\n",
    "                                            step=1,\n",
    "                                            description='Batch Size:',\n",
    "                                            style={'description_width': 'initial'},\n",
    "                                            continuous_update=False)\n",
    "               );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The SVM Algorithm\n",
    "\n",
    "The svm loss is defined as: $$L(w; X, Y) =  \\sum_{i=1}^{N} L_{\\text{svm}} (w; x_i, y_i) =  \\sum_{i=1}^{N} \\max \\{ 0, 1-y_i w^\\top x_i \\}.$$\n",
    "\n",
    "The loss function is continuous, but not differentialbe at $y_i w^\\top x_i=0$. The subgradient, however, exists and hence (stochastic) gradient descent converges. The subgradient is:\n",
    "\n",
    "$$ \\partial L_{\\text{svm}}(w;x_i,y_i) = \\left\\{\\begin{array}{cc} 0 & \\text{if } 1-y_i w^\\top x_i < 0 \\\\ -y_i x_i & \\text{if } 1-y_i w^\\top x_i > 0 \\\\ \\left[0, -y_i x_i \\right] & \\text{if } 1-y_i w^\\top x_i = 0 \\end{array}  \\right. $$\n",
    "\n",
    "The difference with the perceptron loss is that the SVM loss includes a loss margin. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def change_learning_params(reg, eta0, n_iter, batch_size):\n",
    "    classifier = SVM(X_train, Y_train)\n",
    "    classifier.load_test_data(X_test, Y_test)\n",
    "    \n",
    "    regularizer = L2Regularizer(np.power(10., reg))\n",
    "    np.random.seed(42)\n",
    "    w0 = np.random.randn(3, )\n",
    "\n",
    "    opts = {'eta0': eta0,\n",
    "            'n_iter': n_iter,\n",
    "            'batch_size': batch_size,\n",
    "            'n_samples': X_train.shape[0],\n",
    "            'algorithm': 'SGD',\n",
    "            'learning_rate_scheduling': None\n",
    "            }\n",
    "\n",
    "    trajectory, indexes = gradient_descent(w0, classifier, regularizer, opts)\n",
    "    \n",
    "    contour_plot = plt.subplot(121)\n",
    "    error_plot = plt.subplot(122)\n",
    "    \n",
    "    opt = {'marker': 'ro', 'fillstyle': 'full', 'label': '+ Train', 'size': 8}\n",
    "    plot_helpers.plot_data(X_train[np.where(Y_train == 1)[0], 0], X_train[np.where(Y_train == 1)[0], 1], fig=contour_plot, options=opt)\n",
    "    opt = {'marker': 'bs', 'fillstyle': 'full', 'label': '- Train', 'size': 8}\n",
    "    plot_helpers.plot_data(X_train[np.where(Y_train == -1)[0], 0], X_train[np.where(Y_train == -1)[0], 1], fig=contour_plot, options=opt)\n",
    "\n",
    "    opt = {'marker': 'ro', 'fillstyle': 'none', 'label': '+ Test', 'size': 8}\n",
    "    plot_helpers.plot_data(X_test[np.where(Y_test == 1)[0], 0], X_test[np.where(Y_test == 1)[0], 1], fig=contour_plot, options=opt)\n",
    "    opt = {'marker': 'bs', 'fillstyle': 'none', 'label': '- Test', 'size': 8}\n",
    "    plot_helpers.plot_data(X_test[np.where(Y_test == -1)[0], 0], X_test[np.where(Y_test == -1)[0], 1], fig=contour_plot, options=opt)\n",
    "\n",
    "    contour_opts = {'n_points': 100, 'x_label': '$x$', 'y_label': '$y$', 'sgd_point': True, 'n_classes': 4}\n",
    "    error_opts = {'epoch': 5, 'x_label': '$t$', 'y_label': 'error'}\n",
    "    \n",
    "    opts = {'contour_opts': contour_opts, 'error_opts': error_opts}\n",
    "    plot_helpers.classification_progression(X, Y, trajectory, indexes, classifier, \n",
    "                                            contour_plot=contour_plot, error_plot=error_plot, \n",
    "                                            options=opts)\n",
    "\n",
    "interact_manual(change_learning_params,\n",
    "                reg=ipywidgets.FloatSlider(value=-3,\n",
    "                                    min=-3,\n",
    "                                    max=3,\n",
    "                                    step=0.5,\n",
    "                                    readout_format='.1f',\n",
    "                                    description='Regularization 10^:',\n",
    "                                    style={'description_width': 'initial'},\n",
    "                                    continuous_update=False), \n",
    "                eta0=ipywidgets.FloatSlider(value=1,\n",
    "                                            min=1e-1,\n",
    "                                            max=2,\n",
    "                                            step=1 * 1e-1,\n",
    "                                            readout_format='.1f',\n",
    "                                            description='Learning rate:',\n",
    "                                            style={'description_width': 'initial'},\n",
    "                                            continuous_update=False),\n",
    "                n_iter=ipywidgets.IntSlider(value=50,\n",
    "                                            min=5,\n",
    "                                            max=100,\n",
    "                                            step=1,\n",
    "                                            description='Number of iterations:',\n",
    "                                            style={'description_width': 'initial'},\n",
    "                                            continuous_update=False),\n",
    "                batch_size=ipywidgets.IntSlider(value=1,\n",
    "                                            min=1,\n",
    "                                            max=X_train.shape[0],\n",
    "                                            step=1,\n",
    "                                            description='Batch Size:',\n",
    "                                            style={'description_width': 'initial'},\n",
    "                                            continuous_update=False)\n",
    "               );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Logistic Regression Algorithm\n",
    "\n",
    "The logistic loss is defined as: $$L(w; X, Y) = \\sum_{i=1}^{N} L_{\\sigma}(w; x_i, y_i) = \\sum_{i=1}^{N} \\log \\left( 1 + \\exp \\left(-y_i w^\\top x_i \\right) \\right) .$$\n",
    "\n",
    "The loss function is differentiable, hence the gradient is:\n",
    "\n",
    "$$ \\frac{\\partial L_{\\sigma}(w; x_i, y_i)}{\\partial w} = -\\frac{\\exp \\left(-y_i w^\\top x_i \\right)}{1+\\exp \\left(-y_i w^\\top x_i \\right)} y x_i$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def change_learning_params(reg, eta0, n_iter, batch_size):\n",
    "    classifier = Logistic(X_train, Y_train)\n",
    "    classifier.load_test_data(X_test, Y_test)\n",
    "    \n",
    "    regularizer = L2Regularizer(np.power(10., reg))\n",
    "    np.random.seed(42)\n",
    "    w0 = np.random.randn(3, )\n",
    "\n",
    "    opts = {'eta0': eta0,\n",
    "            'n_iter': n_iter,\n",
    "            'batch_size': batch_size,\n",
    "            'n_samples': X_train.shape[0],\n",
    "            'algorithm': 'SGD',\n",
    "            'learning_rate_scheduling': 'AdaGrad'\n",
    "            }\n",
    "\n",
    "    trajectory, indexes = gradient_descent(w0, classifier, regularizer, opts)\n",
    "    \n",
    "    contour_plot = plt.subplot(121)\n",
    "    error_plot = plt.subplot(122)\n",
    "    \n",
    "    opt = {'marker': 'ro', 'fillstyle': 'full', 'label': '+ Train', 'size': 8}\n",
    "    plot_helpers.plot_data(X_train[np.where(Y_train == 1)[0], 0], X_train[np.where(Y_train == 1)[0], 1], fig=contour_plot, options=opt)\n",
    "    opt = {'marker': 'bs', 'fillstyle': 'full', 'label': '- Train', 'size': 8}\n",
    "    plot_helpers.plot_data(X_train[np.where(Y_train == -1)[0], 0], X_train[np.where(Y_train == -1)[0], 1], fig=contour_plot, options=opt)\n",
    "\n",
    "    opt = {'marker': 'ro', 'fillstyle': 'none', 'label': '+ Test', 'size': 8}\n",
    "    plot_helpers.plot_data(X_test[np.where(Y_test == 1)[0], 0], X_test[np.where(Y_test == 1)[0], 1], fig=contour_plot, options=opt)\n",
    "    opt = {'marker': 'bs', 'fillstyle': 'none', 'label': '- Test', 'size': 8}\n",
    "    plot_helpers.plot_data(X_test[np.where(Y_test == -1)[0], 0], X_test[np.where(Y_test == -1)[0], 1], fig=contour_plot, options=opt)\n",
    "\n",
    "    contour_opts = {'n_points': 100, 'x_label': '$x$', 'y_label': '$y$', 'sgd_point': True, 'n_classes': 4}\n",
    "    error_opts = {'epoch': 5, 'x_label': '$t$', 'y_label': 'error'}\n",
    "    \n",
    "    opts = {'contour_opts': contour_opts, 'error_opts': error_opts}\n",
    "    plot_helpers.classification_progression(X, Y, trajectory, indexes, classifier, \n",
    "                                            contour_plot=contour_plot, error_plot=error_plot, \n",
    "                                            options=opts)\n",
    "\n",
    "interact_manual(change_learning_params,\n",
    "                reg=ipywidgets.FloatSlider(value=-3,\n",
    "                                    min=-3,\n",
    "                                    max=3,\n",
    "                                    step=0.5,\n",
    "                                    readout_format='.1f',\n",
    "                                    description='Regularization 10^:',\n",
    "                                    style={'description_width': 'initial'},\n",
    "                                    continuous_update=False), \n",
    "                eta0=ipywidgets.FloatSlider(value=0.5,\n",
    "                                            min=1e-1,\n",
    "                                            max=2,\n",
    "                                            step=1 * 1e-1,\n",
    "                                            readout_format='.1f',\n",
    "                                            description='Learning rate:',\n",
    "                                            style={'description_width': 'initial'},\n",
    "                                            continuous_update=False),\n",
    "                n_iter=ipywidgets.IntSlider(value=100,\n",
    "                                            min=5,\n",
    "                                            max=50,\n",
    "                                            step=1,\n",
    "                                            description='Number of iterations:',\n",
    "                                            style={'description_width': 'initial'},\n",
    "                                            continuous_update=False),\n",
    "                batch_size=ipywidgets.IntSlider(value=1,\n",
    "                                            min=1,\n",
    "                                            max=X_train.shape[0],\n",
    "                                            step=1,\n",
    "                                            description='Batch Size:',\n",
    "                                            style={'description_width': 'initial'},\n",
    "                                            continuous_update=False)\n",
    "               );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernelized Classification\n",
    "\n",
    "Both perceptron and SVM can be kernelized, i.e. the output of the regressor can be passed through the sign function. \n",
    "\n",
    "$$ \\hat{y} = \\text{sign} \\left[ \\sum_{i=1}^N \\alpha_i y_i K(x_i, x) \\right] $$ and a perceptron loss applied to the difference between the predictor and the true value. \n",
    "\n",
    "The perceptron loss can be posed as:\n",
    "\n",
    "$$ L(\\alpha;x_j, y_j) = \\max \\left\\{0, -\\sum_{j=1}^N y_j \\alpha_i K(x_i, x_j) \\right\\} $$ \n",
    "\n",
    "The parameters are updated as follows: \n",
    "If $\\hat{y}(x) \\neq y(x)$, set $\\alpha \\gets \\alpha + \\eta_t$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_points = 100  # Number of points per class\n",
    "noise = 0.2  # Noise Level (needed for data generation).\n",
    "\n",
    "X, Y = generate_circular_separable_data(num_points, noise=noise, offset=1)\n",
    "fig = plt.subplot(111)\n",
    "opt = {'marker': 'r*', 'label': '+'}\n",
    "plot_helpers.plot_data(X[np.where(Y == 1)[0], 0], X[np.where(Y == 1)[0], 1], fig=fig, options=opt)\n",
    "opt = {'marker': 'bs', 'label': '-'}\n",
    "plot_helpers.plot_data(X[np.where(Y == -1)[0], 0], X[np.where(Y == -1)[0], 1], fig=fig, options=opt)\n",
    "fig.legend();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kernels import PolynomialKernel, LinearKernel, LaplacianKernel, GaussianKernel, GaussianLinearKernel, PeriodicKernel, PeriodicLinearKernel\n",
    "\n",
    "def change_learning_params(eta0, n_iter, batch_size, kernel, deg):\n",
    "    if kernel == 'Linear':\n",
    "        classifier = PolynomialKernel(X, Y, reg=0.00, deg=1, prediction=False)\n",
    "    elif kernel == 'Polynomial':\n",
    "        classifier = PolynomialKernel(X, Y, reg=0.00, deg=deg, prediction=False)\n",
    "    elif kernel == 'Gaussian':\n",
    "        classifier = GaussianKernel(X, Y, reg=0.00, bw=0.2, prediction=False)   \n",
    "    elif kernel == 'Laplacian':\n",
    "        classifier = LaplacianKernel(X, Y, reg=0.00, bw=0.2, prediction=False)  \n",
    "    \n",
    "    regularizer = L2Regularizer(0.)\n",
    "    \n",
    "    alpha0 = 0 * np.random.randn(X.shape[0])\n",
    "\n",
    "    opts = {'eta0': eta0,\n",
    "            'n_iter': n_iter,\n",
    "            'batch_size': batch_size,\n",
    "            'n_samples': X.shape[0],\n",
    "            'algorithm': 'SGD',\n",
    "            'learning_rate_scheduling': None\n",
    "            }\n",
    "    alphas, indexes = gradient_descent(alpha0, classifier, regularizer, opts)\n",
    "\n",
    "    fig = plt.subplot(111)\n",
    "    opt = {'marker': 'r*', 'label': '+'}\n",
    "    plot_helpers.plot_data(X[np.where(Y == 1)[0], 0], X[np.where(Y == 1)[0], 1], fig=fig, options=opt)\n",
    "    opt = {'marker': 'bs', 'label': '-'}\n",
    "    plot_helpers.plot_data(X[np.where(Y == -1)[0], 0], X[np.where(Y == -1)[0], 1], fig=fig, options=opt)\n",
    "\n",
    "    contour_opts = {'n_points': 20, 'x_label': '$x$', 'y_label': '$y$', 'sgd_point': False, 'n_classes': 2}\n",
    "    opts = {'contour_opts': contour_opts}\n",
    "    plot_helpers.classification_progression(X, Y, alphas, indexes, classifier, contour_plot=fig, options=opts)\n",
    "\n",
    "interact_manual(change_learning_params,\n",
    "                eta0=ipywidgets.FloatSlider(value=0.5,\n",
    "                                            min=1e-1,\n",
    "                                            max=2,\n",
    "                                            step=1 * 1e-1,\n",
    "                                            readout_format='.1f',\n",
    "                                            description='Learning rate:',\n",
    "                                            style={'description_width': 'initial'},\n",
    "                                            continuous_update=False),\n",
    "                n_iter=ipywidgets.IntSlider(value=20,\n",
    "                                            min=5,\n",
    "                                            max=50,\n",
    "                                            step=1,\n",
    "                                            description='Number of iterations:',\n",
    "                                            style={'description_width': 'initial'},\n",
    "                                            continuous_update=False),\n",
    "                batch_size=ipywidgets.IntSlider(value=X.shape[0],\n",
    "                                            min=1,\n",
    "                                            max=X.shape[0],\n",
    "                                            step=1,\n",
    "                                            description='Batch Size:',\n",
    "                                            style={'description_width': 'initial'},\n",
    "                                            continuous_update=False),\n",
    "                kernel=ipywidgets.RadioButtons(\n",
    "                             options=['Polynomial', 'Gaussian', 'Laplacian'],\n",
    "                             value='Polynomial',\n",
    "                             description='Kernel type:',\n",
    "                             style={'description_width': 'initial'}),\n",
    "                deg = ipywidgets.IntSlider(\n",
    "                         value=1,\n",
    "                         min=1,\n",
    "                         max=10, \n",
    "                         step=1,\n",
    "                         description='Degree of Polynomial kernel:',\n",
    "                         style={'description_width': 'initial'}),\n",
    "               );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM KERNELS\n",
    "The SVM can also be kernelized! let's look at an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def laplacian_kernel(X, Y, bw):\n",
    "    rows = X.shape[0]\n",
    "    cols = Y.shape[0]\n",
    "    K = np.zeros((rows, cols))\n",
    "    for col in range(cols):\n",
    "        dist = bw * np.linalg.norm(X - Y[col, :], ord=1, axis=1)\n",
    "        K[:, col] = np.exp(-dist)\n",
    "    return K\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn import datasets\n",
    "import sklearn\n",
    "\n",
    "# Our dataset and targets\n",
    "n_samples = 200  # Number of points per class\n",
    "tol = 1e-1\n",
    "# noise = 0.2  # Noise Level (needed for data generation).\n",
    "\n",
    "# X, Y = generate_circular_separable_data(num_points, noise=noise, offset=1)\n",
    "# X = X[:, 0:2]\n",
    "\n",
    "def svm_kernel(dataset, kernel, reg, bw, deg, noise):\n",
    "    if dataset is 'blobs':\n",
    "        X, Y = datasets.make_blobs(n_samples=n_samples, centers=2, random_state=3, cluster_std=10*noise)\n",
    "    elif dataset is 'circles':\n",
    "        X, Y = datasets.make_circles(n_samples=n_samples, factor=.5, noise=noise, random_state=42)\n",
    "    elif dataset is 'moons':\n",
    "        X, Y = datasets.make_moons(n_samples=n_samples, noise=noise, random_state=42)\n",
    "    elif dataset == 'xor':\n",
    "        np.random.seed(42)\n",
    "        step = int(n_samples/4)\n",
    "        \n",
    "        X = np.zeros((n_samples, 2))\n",
    "        Y = np.zeros(n_samples)\n",
    "        \n",
    "        X[0*step:1*step, :] = noise * np.random.randn(step, 2)\n",
    "        Y[0*step:1*step] = 1\n",
    "        X[1*step:2*step, :] = np.array([1, 1]) + noise * np.random.randn(step, 2)\n",
    "        Y[1*step:2*step] = 1\n",
    "        \n",
    "        X[2*step:3*step, :] = np.array([0, 1]) + noise * np.random.randn(step, 2)\n",
    "        Y[2*step:3*step] = -1\n",
    "        X[3*step:4*step, :] = np.array([1, 0]) + noise * np.random.randn(step, 2)\n",
    "        Y[3*step:4*step] = -1\n",
    "    \n",
    "    elif dataset == 'periodic':\n",
    "        np.random.seed(42)\n",
    "        step = int(n_samples/4)\n",
    "        \n",
    "        X = np.zeros((n_samples, 2))\n",
    "        Y = np.zeros(n_samples)\n",
    "        \n",
    "        X[0*step:1*step, :] = noise * np.random.randn(step, 2)\n",
    "        Y[0*step:1*step] = 1\n",
    "        X[1*step:2*step, :] = np.array([0, 2]) + noise * np.random.randn(step, 2)\n",
    "        Y[1*step:2*step] = 1\n",
    "        \n",
    "        X[2*step:3*step, :] = np.array([0, 1]) + noise * np.random.randn(step, 2)\n",
    "        Y[2*step:3*step] = -1\n",
    "        X[3*step:4*step, :] = np.array([0, 3]) + noise * np.random.randn(step, 2)\n",
    "        Y[3*step:4*step] = -1\n",
    "        \n",
    "    X = X[Y <= 1, :]\n",
    "    Y = Y[Y <=1 ]\n",
    "    Y[Y==0] = -1\n",
    "        \n",
    "    # Add the 1 feature.  \n",
    "    X = np.concatenate((X, np.ones((X.shape[0], 1))), axis=1)\n",
    "    plot_support = True\n",
    "    if kernel == 'poly':\n",
    "        gamma = 1\n",
    "        coef0 = 0\n",
    "    elif kernel == 'sigmoid':\n",
    "        gamma = np.power(10., bw)\n",
    "        coef0 = 0\n",
    "    elif kernel == 'rbf':\n",
    "        gamma = np.power(10., -bw)\n",
    "        coef0 = 0\n",
    "    elif kernel == 'laplacian':\n",
    "        gamma = np.power(10., -bw)\n",
    "        coef0 = 0\n",
    "        kernel = lambda X, Y: laplacian_kernel(X, Y, gamma)\n",
    "        plot_support = False\n",
    "    \n",
    "    classifier = svm.SVC(kernel=kernel, C=np.power(10., -reg), gamma=gamma, degree=deg, coef0=coef0, tol=tol)\n",
    "    classifier.fit(X, Y)\n",
    "\n",
    "    # plot the line, the points, and the nearest vectors to the plane\n",
    "    plt.figure()\n",
    "    plt.clf()\n",
    "    fig = plt.axes()\n",
    "    opt = {'marker': 'r*', 'label': '+'}\n",
    "    plot_helpers.plot_data(X[np.where(Y == 1)[0], 0], X[np.where(Y == 1)[0], 1], fig=fig, options=opt)\n",
    "    opt = {'marker': 'bs', 'label': '-'}\n",
    "    plot_helpers.plot_data(X[np.where(Y == -1)[0], 0], X[np.where(Y == -1)[0], 1], fig=fig, options=opt)\n",
    "    \n",
    "    if plot_support:\n",
    "        plt.scatter(classifier.support_vectors_[:, 0], classifier.support_vectors_[:, 1], s=80,\n",
    "                    facecolors='none', edgecolors='k')\n",
    "\n",
    "    mins = np.min(X, 0)\n",
    "    maxs = np.max(X, 0)\n",
    "    x_min = mins[0] - 1\n",
    "    x_max = maxs[0] + 1\n",
    "    y_min = mins[1] - 1\n",
    "    y_max = maxs[1] + 1\n",
    "\n",
    "    XX, YY = np.mgrid[x_min:x_max:200j, y_min:y_max:200j]  \n",
    "    Xtest = np.c_[XX.ravel(), YY.ravel(), np.ones_like(XX.ravel())]\n",
    "    # Xtest = np.c_[XX.ravel(), YY.ravel()]\n",
    "    Z = classifier.decision_function(Xtest)\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(XX.shape)\n",
    "    # plt.figure(fignum, figsize=(4, 3))\n",
    "    plt.contourf(XX, YY, Z > 0, cmap=plt.cm.jet, alpha=0.3)\n",
    "    plt.contour(XX, YY, Z, colors=['k', 'k', 'k'], linestyles=['--', '-', '--'], levels=[-.99, 0, .99])\n",
    "\n",
    "    plt.xlim(x_min, x_max)\n",
    "    plt.ylim(y_min, y_max)\n",
    "\n",
    "#     plt.xticks(())\n",
    "#     plt.yticks(())\n",
    "\n",
    "interact_manual(svm_kernel, \n",
    "        dataset=['blobs', 'circles', 'moons', 'xor', 'periodic'],\n",
    "        kernel=['poly', 'rbf', 'laplacian'], \n",
    "        reg=ipywidgets.FloatSlider(value=-3,\n",
    "                                    min=-3,\n",
    "                                    max=3,\n",
    "                                    step=0.5,\n",
    "                                    readout_format='.1f',\n",
    "                                    description='Regularization 10^:',\n",
    "                                    style={'description_width': 'initial'},\n",
    "                                    continuous_update=False),\n",
    "        bw=ipywidgets.FloatSlider(value=-1,\n",
    "                                    min=-3,\n",
    "                                    max=3,\n",
    "                                    step=0.1,\n",
    "                                    readout_format='.1f',\n",
    "                                    description='Bandwidth 10^:',\n",
    "                                    style={'description_width': 'initial'},\n",
    "                                    continuous_update=False),  \n",
    "        deg=ipywidgets.IntSlider(\n",
    "                         value=1,\n",
    "                         min=1,\n",
    "                         max=10, \n",
    "                         step=1,\n",
    "                         description='Degree of Poly:',\n",
    "                         style={'description_width': 'initial'}),\n",
    "        noise=ipywidgets.FloatSlider(value=0.05,\n",
    "                                    min=0.01,\n",
    "                                    max=0.3,\n",
    "                                    step=0.01,\n",
    "                                    readout_format='.2f',\n",
    "                                    description='Noise level:',\n",
    "                                    style={'description_width': 'initial'},\n",
    "                                    continuous_update=False),  \n",
    "        );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN\n",
    "\n",
    "The K-Nearest Neighbors classifier is very easy. The label of a next sample is the label most voted by the $k$ training samples that are closer to this sample.\n",
    "\n",
    "A simple implementation is $$\\hat{y} = \\text{sign}\\left\\{\\sum_{i \\in \\mathcal{N}_k (x)} K(x_i, x) y_i\\right\\},$$\n",
    "\n",
    "where $\\mathcal{N}_k (x)$ is the set with the $k$ closest neighbours of $x$, $K(x_i, x)$ is a weighting coefficient, and $y_i$ is the label of example $i$. Usually $K(\\cdot, \\cdot)$ can be a kernel function, that measures the similarity between two points. In the vanilla k-NN method the kernel is just the identity function. \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from classifiers import kNN\n",
    "\n",
    "noise = 0.4\n",
    "X, Y = generate_circular_separable_data(num_points, noise=noise, offset=1)\n",
    "\n",
    "def change_k_nn(k):\n",
    "    classifier = kNN(X, Y, k)\n",
    "    fig = plt.subplot(111)\n",
    "    opt = {'marker': 'r*', 'label': '+'}\n",
    "    plot_helpers.plot_data(X[np.where(Y == 1)[0], 0], X[np.where(Y == 1)[0], 1], fig=fig, options=opt)\n",
    "    opt = {'marker': 'bs', 'label': '-'}\n",
    "    plot_helpers.plot_data(X[np.where(Y == -1)[0], 0], X[np.where(Y == -1)[0], 1], fig=fig, options=opt)\n",
    "\n",
    "    opt = {'n_points': 20, 'x_label': '$x$', 'y_label': '$y$'}\n",
    "    plot_helpers.plot_classification_boundaries(X, classifier, fig=fig, options=opt)\n",
    "\n",
    "interact(change_k_nn,\n",
    "         k=ipywidgets.IntSlider(value=1,\n",
    "                                min=1,\n",
    "                                max=9,\n",
    "                                step=2,\n",
    "                                description='k',\n",
    "                                style={'description_width': 'initial'},\n",
    "                                continuous_update=False)\n",
    "         );\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Cost Sensitivity\n",
    "\n",
    "When there are more positive than negative examples the then, to overcome this uneven distribution of data, the cost of mislabeling can be changed between classes. \n",
    "\n",
    "The cost can be expressed as:  $$ w^\\star = \\arg \\min_w \\sum_{i=1}^{N} c(y_i) \\left[y_i\\neq \\text{sign} (w^\\top x_i) \\right], $$ \n",
    "where $c(y_i)$ is a cost that depends on the class. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_positive = 20  # Number of points per class\n",
    "num_negative = 100  # Number of points per class\n",
    "noise = 0.3  # Noise Level (needed for data generation).\n",
    "\n",
    "X, Y = generate_linear_separable_data(num_positive, num_negative, noise=noise, dim=2)\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "def cost_sensitivity(algorithm, ratio):\n",
    "    class_weight = {-1: 1, 1:np.power(10., ratio)}\n",
    "    if algorithm == 'SVM':\n",
    "        classifier = LinearSVC(class_weight=class_weight)\n",
    "    elif algorithm == 'Perceptron':\n",
    "        classifier = SGDClassifier(loss='perceptron', random_state=1, class_weight=class_weight)\n",
    "\n",
    "    classifier.fit(X[:,:2], Y)\n",
    "    \n",
    "    X0, X1 = X[:, 0], X[:, 1]\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    h = .02\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "    fig = plt.subplot(111)\n",
    "    plot_helpers.plot_contours(fig, classifier, xx, yy, cmap=plt.cm.jet, alpha=0.3)\n",
    "    opt = {'marker': 'r*', 'label': '+'}\n",
    "    plot_helpers.plot_data(X[np.where(Y == 1)[0], 0], X[np.where(Y == 1)[0], 1], fig=fig, options=opt)\n",
    "    opt = {'marker': 'bs', 'label': '-'}\n",
    "    plot_helpers.plot_data(X[np.where(Y == -1)[0], 0], X[np.where(Y == -1)[0], 1], fig=fig, options=opt)\n",
    "\n",
    "    \n",
    "interact(cost_sensitivity,\n",
    "         algorithm=ipywidgets.RadioButtons(\n",
    "             options=['SVM', 'Perceptron'],\n",
    "             value='SVM',\n",
    "             description='Algorithm:',\n",
    "             style={'description_width': 'initial'}),\n",
    "         ratio=ipywidgets.FloatSlider(\n",
    "             value=0,\n",
    "             min=-1,\n",
    "             max=3,\n",
    "             step=0.5,\n",
    "             description='Cost Ratio 10^:',\n",
    "             style={'description_width': 'initial'},\n",
    "             continuous_update=False)\n",
    "         );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L1 Regularization\n",
    "\n",
    "The L-1 regularization method uses a regularizer of the form $R(w) = ||w||_1$ which is non-differentiable. However, the subgradient exits and is:\n",
    "\n",
    "$$\\partial(||w|||_1) = \\left\\{\\begin{array}{cc} \\text{sign} (w)& \\text{if } w \\neq 0 \\\\ [-1, 1]  & \\text{if } w = 0 \\end{array} \\right. $$\n",
    "\n",
    "This regularization method penalizes weights and induces sparsity in the solutions. That is, most of the entries of the solution $w^\\star$ will be zero. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_positive = 50  # Number of points per class\n",
    "num_negative = 50  # Number of points per class\n",
    "\n",
    "noise = 0.3  # Noise Level (needed for data generation).\n",
    "\n",
    "X, Y = generate_linear_separable_data(num_positive, num_negative, offset=np.array([1, .2]), noise=noise, dim=2)\n",
    "X = X - np.mean(X, axis=0)\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "def regularization(regularizer, reg):\n",
    "    np.random.seed(42)\n",
    "    classifier = SGDClassifier(loss='perceptron', penalty=regularizer, alpha = np.power(10., reg), random_state=1)\n",
    "    classifier.fit(X[:,:2], Y)\n",
    "    \n",
    "    X0, X1 = X[:, 0], X[:, 1]\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    h = .02\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "    fig = plt.subplot(111)\n",
    "    contour = plot_helpers.plot_contours(fig, classifier, xx, yy, cmap=plt.cm.jet, alpha=0.3)\n",
    "    plt.colorbar(contour)\n",
    "    opt = {'marker': 'r*', 'label': '+'}\n",
    "    plot_helpers.plot_data(X[np.where(Y == 1)[0], 0], X[np.where(Y == 1)[0], 1], fig=fig, options=opt)\n",
    "    opt = {'marker': 'bs', 'label': '-', 'legend': True}\n",
    "    plot_helpers.plot_data(X[np.where(Y == -1)[0], 0], X[np.where(Y == -1)[0], 1], fig=fig, options=opt)\n",
    "    \n",
    "interact(regularization,\n",
    "         regularizer=ipywidgets.RadioButtons(\n",
    "             options=['l1', 'l2'],\n",
    "             value='l1',\n",
    "             description='Algorithm:',\n",
    "             style={'description_width': 'initial'}),\n",
    "         reg=ipywidgets.FloatSlider(\n",
    "             value=-3,\n",
    "             min=-3,\n",
    "             max=0,\n",
    "             step=0.5,\n",
    "             description='Regularizer 10^:',\n",
    "             style={'description_width': 'initial'},\n",
    "             continuous_update=False)\n",
    "         );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Multiclass Classification\n",
    "\n",
    "In multiclass classification there are two strategies: OvO (One vs One) or OvR (One vs The Rest). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsOneClassifier, OneVsRestClassifier\n",
    "from sklearn import datasets\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "np.random.seed(1)\n",
    "iris = datasets.load_iris()\n",
    "X, y = iris.data[:,:2], iris.target\n",
    "mean = X.mean(axis=0)\n",
    "std = X.std(axis=0)\n",
    "X = (X - mean) / std\n",
    "\n",
    "def multiclass(strategy):\n",
    "    if strategy == 'OvO':\n",
    "        classifier =  OneVsOneClassifier(SGDClassifier(loss='perceptron', alpha=0.001, n_iter=100, random_state=0))\n",
    "#         classifier =  OneVsOneClassifier(LinearSVC(random_state=0))\n",
    "        colors = ['g', 'r', 'b']\n",
    "    elif strategy == 'OvR' or strategy == 'OvA':\n",
    "        classifier =  OneVsRestClassifier(SGDClassifier(loss='perceptron', alpha=0.001, n_iter=100, random_state=0))\n",
    "#         classifier =  OneVsRestClassifier(LinearSVC(random_state=0))\n",
    "        colors = ['b', 'g', 'r']\n",
    "        \n",
    "\n",
    "    classifier.fit(X, y)\n",
    "    X0, X1 = X[:, 0], X[:, 1]\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    h = .02\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "    fig = plt.subplot(111)\n",
    "    plot_helpers.plot_contours(fig, classifier, xx, yy, cmap=plt.cm.jet, alpha=0.3)\n",
    "    \n",
    "    opt = {'marker': 'bs', 'label': '-', 'x_label': '$x$', 'y_label': '$y$', 'size': 8}\n",
    "    plot_helpers.plot_data(X[np.where(y == 0)[0], 0], X[np.where(y == 0)[0], 1], fig=fig, options=opt)\n",
    "    opt = {'marker': 'g*', 'label': '+', 'size': 8}\n",
    "    plot_helpers.plot_data(X[np.where(y == 1)[0], 0], X[np.where(y == 1)[0], 1], fig=fig, options=opt)\n",
    "    opt = {'marker': 'ro', 'label': '-', 'x_label': '$x$', 'y_label': '$y$', 'size': 8, 'legend': True}\n",
    "    plot_helpers.plot_data(X[np.where(y == 2)[0], 0], X[np.where(y == 2)[0], 1], fig=fig, options=opt)\n",
    "    \n",
    "\n",
    "    def plot_hyperplane(clf, color):\n",
    "        coef = clf.coef_\n",
    "        intercept = clf.intercept_\n",
    "        def line(x0):\n",
    "            return (-(x0 * coef[0, 0]) - intercept[0]) / coef[0, 1]\n",
    "\n",
    "        plt.plot([x_min, x_max], [line(x_min), line(x_max)],\n",
    "                 ls=\"--\", color=color)\n",
    "\n",
    "    \n",
    "    for estimator, color in zip(classifier.estimators_, colors):\n",
    "        plot_hyperplane(estimator, color)\n",
    "\n",
    "    fig.set_xlim([x_min, x_max])\n",
    "    fig.set_ylim([y_min, y_max]);\n",
    "\n",
    "interact(multiclass, strategy=['OvO', 'OvA']); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiclass with Nearest Neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import neighbors\n",
    "\n",
    "np.random.seed(1)\n",
    "iris = datasets.load_iris()\n",
    "X, y = iris.data[:,:2], iris.target\n",
    "\n",
    "def change_n_neighbors(n_neighbors):\n",
    "    for weights in ['uniform']:#, 'distance']:\n",
    "        # we create an instance of Neighbours Classifier and fit the data.\n",
    "        clf = neighbors.KNeighborsClassifier(n_neighbors, weights=weights)\n",
    "        clf.fit(X, y)\n",
    "\n",
    "        # Plot the decision boundary. For that, we will assign a color to each\n",
    "        # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "        h = 0.1\n",
    "        x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "        y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "        xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                             np.arange(y_min, y_max, h))\n",
    "        Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "        # Put the result into a color plot\n",
    "        Z = Z.reshape(xx.shape)\n",
    "        plt.figure()\n",
    "        fig = plt.axes()\n",
    "        plt.contourf(xx, yy, Z, cmap=plt.cm.jet, alpha=0.3)\n",
    "        \n",
    "        # Plot also the training points\n",
    "        opt = {'marker': 'bs', 'label': '-', 'x_label': '$x$', 'y_label': '$y$', 'size': 8}\n",
    "        plot_helpers.plot_data(X[np.where(y == 0)[0], 0], X[np.where(y == 0)[0], 1], fig=fig, options=opt)\n",
    "        opt = {'marker': 'g*', 'label': '+', 'size': 8}\n",
    "        plot_helpers.plot_data(X[np.where(y == 1)[0], 0], X[np.where(y == 1)[0], 1], fig=fig, options=opt)\n",
    "        opt = {'marker': 'ro', 'label': '-', 'x_label': '$x$', 'y_label': '$y$', 'size': 8, 'legend': True}\n",
    "        plot_helpers.plot_data(X[np.where(y == 2)[0], 0], X[np.where(y == 2)[0], 1], fig=fig, options=opt)\n",
    "        \n",
    "        plt.xlim(xx.min(), xx.max())\n",
    "        plt.ylim(yy.min(), yy.max())\n",
    "        plt.title(\"3-Class classification (k = %i, weights = '%s')\"\n",
    "                  % (n_neighbors, weights))\n",
    "    \n",
    "interact(change_n_neighbors, n_neighbors=ipywidgets.IntSlider(value=1, min=1, max=15, step=2));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "widgets": {
   "state": {
    "1dbcd274dcfe470b91708912892ed69f": {
     "views": [
      {
       "cell_index": 18
      }
     ]
    },
    "2cfdd1a03116452aa5629c3d90011059": {
     "views": [
      {
       "cell_index": 13
      }
     ]
    },
    "4e37636106d041fab7a6ff19b8cb36c7": {
     "views": [
      {
       "cell_index": 15
      }
     ]
    },
    "674c84c930124a6f95d134bf447ea1c1": {
     "views": [
      {
       "cell_index": 22
      }
     ]
    },
    "85cd0d5f4f1e4750ab3b284e95bc5242": {
     "views": [
      {
       "cell_index": 24
      }
     ]
    },
    "a44fb013255548a7b20262200fd53b86": {
     "views": [
      {
       "cell_index": 20
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
